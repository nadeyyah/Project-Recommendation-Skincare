# -*- coding: utf-8 -*-
"""project-recommendation-skincare.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-0zXJtnLAsLHqTNE_2ENpcWqGQut0ySg

# Import Libary

*Script* di bawah ini bertujuan untuk melakukan *import* atau memanggil *library* untuk membantu analisis, terdiri atas *data manipulation*, *data visualization*, *modeling*, dan *loading data* eksternal dari *kaggle*.
"""

# Installing some library
!pip install scikit-learn scipy tabulate

# Data manipulation
import pandas as pd
import numpy as np
import os
# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns
# Modeling (Machine Learning)
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import hstack
from tabulate import tabulate
#Evaluation
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix,accuracy_score
from sklearn.metrics import mean_squared_error, mean_absolute_error
# Loading data
import kagglehub
#ignored warining
import warnings
warnings.filterwarnings('ignore')

"""# Loading Dataset

*Loading dataset* merupakan proses meng*import* dan membaca *dataset*. *dataset* bisa berasal dari internal tersimpan dari dalam perangkat maupun *external* dari link penyedia data luring seperi *kaggle*.
"""

# Import datasetfrom kaggle
dataset_name = "nadyinky/sephora-products-and-skincare-reviews"
location = kagglehub.dataset_download(dataset_name)
files = os.listdir(location)
print(files)

# Loading product csv
csv_file_product = os.path.join(location, "product_info.csv")
product_df = pd.read_csv(csv_file_product)
product_df.head()

# Loading reviews csv
csv_file_reviews = os.path.join(location, "reviews_1250-end.csv")
reviews_df = pd.read_csv(csv_file_reviews)
reviews_df.head()

"""# Data Undestanding

Bagian data understanding berfungsi sebagai bagian untuk mengetahui dimensi dataset, tipe data dari setiap fitur, informasi tentang missing value dan data terduplikasi, serta pemilihan fitur yang relevan.

### Product Dataset
"""

# Product Dataset Dimension
product_df.shape

# Product Dataset Data Type Information
product_df.info()

# Checking for missing value
(product_df.isnull() | product_df.isna() | (product_df == "")).sum()

# Checking for duplicated value
product_df.duplicated().sum()

# Showing uniqe value each features
product_df.nunique()

"""### Reviews Dataset"""

# Reviews Dataset Dimension
reviews_df.shape

# Reviews Dataset Data Type Information
reviews_df.info()

# Checking for missing value
(reviews_df.isnull() |reviews_df.isna() | (reviews_df == "")).sum()

# Checking for duplicated date
reviews_df.duplicated().sum()

# Showing uniqe value each features
reviews_df.nunique()

"""## Exploratory Data Analysis (EDA)

Exploratory Data Analysis (EDA) adalah proses untuk memahami karakteristik, pola, dan struktur data sebelum melakukan analisis lebih lanjut. EDA membantu mengungkap hubungan antar variabel, baik secara statistik maupun visual, guna memberikan wawasan awal untuk membangun model prediktif. Teknik yang umum digunakan meliputi visualisasi, ringkasan statistik, dan korelasi antar variabel.

### Product Dataset
"""

# Top 5 Most Dominating Product Type
primary_category_counts = product_df['primary_category'].value_counts().head(5)
fig, axs = plt.subplots(1, 1, figsize=(10, 6))
axs.bar(primary_category_counts.index, primary_category_counts.values, color='skyblue')
axs.set_xlabel('Primary Category', fontsize=12)
axs.set_ylabel('Count', fontsize=12)
axs.set_title('Top 5 Most Dominating Product Types by Primary Category', fontsize=14)
axs.tick_params(axis='x', rotation=70)
axs.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Top 5 Brand Dominations
brand_counts = product_df['brand_name'].value_counts().head(5)
fig, axs = plt.subplots(1, 1, figsize=(10, 6))
axs.bar(brand_counts.index, brand_counts.values, color='skyblue')
axs.set_xlabel('Brand Name', fontsize=12)
axs.set_ylabel('Count', fontsize=12)
axs.set_title('Top 5 Brand Dominations', fontsize=14)
axs.tick_params(axis='x', rotation=70)
axs.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Top 5 Ingredients Dominations
ingredients_counts = product_df['ingredients'].str.strip('[]').str.replace("'", "").str.split(', ').explode().value_counts().head(5)
fig, axs = plt.subplots(1, 1, figsize=(10, 6))
axs.bar(ingredients_counts.index, ingredients_counts.values, color='skyblue')
axs.set_xlabel('Ingredients Name', fontsize=12)
axs.set_ylabel('Count', fontsize=12)
axs.set_title('Top 5 Ingredients Dominations', fontsize=14)
axs.tick_params(axis='x', rotation=70)
axs.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

"""Berdasarkan EDA yang telah dilakukan pada dataset product, diperoleh bila:
- jenis produk primer yang paling banyak dijual pada situs sephora adalah skincare, Makeup, Fregrance, Hair, bath & body mist.
- brand yang paling banyak dijual adalah shepora collection, clinique, dior, kÃ¨rastase, dan tarte.
- ingredients yang paling banyak dikandung oleh produk yang di jual pada toko ini adalah glycerin, phenoxythanol, caprylyl glycol, limonene, dan tocophenol.

### Reviews Dataset
"""

# Ratings Distribution
rating_counts = reviews_df['rating'].value_counts().sort_index()
fig, axs = plt.subplots(1, 1, figsize=(10, 6))
axs.bar(rating_counts.index, rating_counts.values, color='skyblue')
axs.set_xlabel('Rating', fontsize=12)
axs.set_ylabel('Count', fontsize=12)
axs.set_title('Ratings Distribution', fontsize=14)
axs.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Distribution of Average Ratings by Authors
average_rating_per_author = reviews_df.groupby('author_id')['rating'].mean().reset_index()
average_rating_per_author.rename(columns={'rating': 'average_rating'}, inplace=True)
plt.figure(figsize=(10, 6))
plt.hist(average_rating_per_author['average_rating'], bins=20, color='skyblue', edgecolor='black', alpha=0.7)
plt.title('Distribution of Average Ratings by Authors', fontsize=14)
plt.xlabel('Average Rating', fontsize=12)
plt.ylabel('Number of Authors', fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Average Rating for Top 5 Most Reviewed Products
most_reviewed_products = reviews_df['product_id'].value_counts().head(5)
top_products_df = reviews_df[reviews_df['product_id'].isin(most_reviewed_products.index)]
average_rating_top_products = top_products_df.groupby('product_id')['rating'].mean()
plt.figure(figsize=(8, 5))
plt.bar(average_rating_top_products.index, average_rating_top_products.values, color='lightgreen')
plt.title('Average Rating for Top 5 Most Reviewed Products', fontsize=14)
plt.xlabel('Product ID', fontsize=12)
plt.ylabel('Average Rating', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""Berdasarkan EDA yang sudah dilakukan pada dataset reviews,
- rating 5 adalah rating yang paling sering diberikan oleh author pada saat memberikan penilaian produk, jarang sekali author memberikan rating 2 pada penilaian produknya.
- lima produk dengan jumlah ulasan terbanyak semuanya memiliki rata-rata penilaian yang tinggi, yaitu di atas 4,5.

# Data Preparation

### Product Dataset (Content-Based Filtering)
"""

# Using selection feature for product dataset
product_df = product_df[['product_id','product_name','brand_name','ingredients','primary_category','tertiary_category']]

"""Informasi awal dataset untuk product memiliki dimensi (8494,27) yang artinya 8494 baris dengan 27 kolom (fitur). Dataset product ini berisikan produk yang dijual pada situs sephora, kandungan produk, jenis produk, dan lain sebagainya. Dari 27 fitur, dipilih 5 fitur yang relevan, antara lain

```
'product_id','product_name','brand_name','ingredients','primary_category','tertiary_category'
```
Fitur-fitur ini dipilih dengan alasan yang mendukung analisis produk skincare secara komprehensif.
- **Product_ID** digunakan sebagai nilai unik untuk membedakan setiap produk secara individual.
- **product_name** mencirikan nama produk,
- **brand_name** mencirikan brand yang memproduksi skincare.
- **Ingredients** berperan penting dalam menelusuri kandungan skincare untuk memastikan kecocokan dengan kebutuhan spesifik pengguna, seperti kulit sensitif atau berjerawat.
- **Primary_Category** mencerminkan karakteristik utama produk, misalnya sebagai pewangi atau lotion, sedangkan
- **Tertiary_Category** memberikan informasi tambahan yang lebih spesifik untuk melengkapi kategori utama dan memberikan pemahaman yang lebih rinci tentang produk. Dataframe product ini digunakan untuk sistem rekomendasi **content-based filtering**


"""

# Checking for duplicate value
product_df.duplicated().sum()

# Checking for missing value
(product_df.isnull() | product_df.isna() | (product_df == "")).sum()

# Droping missing value
product_df.dropna(inplace=True)

# Product df shape
product_df.shape

# Product_df unique
product_df.nunique()

"""Setelah dilakukan pemilihan dilanjutkan dengan pemeriksaan nilai yang duplikat serta nilai yang hilang. Terdapat beberapa nilai yang hilang dari dataset, sehingga diputuskan untuk melakukan drop pada row. Setelah dilakukan pembersian tersisa 6642 baris dengan **product_name** sebanyak 6592, **ingredient** sebanyak 5962, 7 **primary_category**, **brand_name** sebanyak 278 dan 106 **tertiary_category** yang  masing- masing bernilai unik."""

# Vectorizer
vectorizer = TfidfVectorizer()

# Making vector word for each feature
ingredient_vector = vectorizer.fit_transform(product_df['ingredients'])
prim_category_vector = vectorizer.fit_transform(product_df['primary_category'])
tert_category_vector = vectorizer.fit_transform(product_df['tertiary_category'])

# Combining all features that have been vectorized
combined_features = hstack([ingredient_vector, prim_category_vector, tert_category_vector])

"""Langkah- langkah yang dilakukan pada data preparation untuk product dataset adalah sebagai berikut:
- Inisialisasi Vectorizer:
Membuat objek TfidfVectorizer() yang akan digunakan untuk mengubah data teks menjadi representasi numerik (vektor).

- Membuat Vektor untuk Setiap Fitur:
Mengubah kolom **ingredients**, **primary_category**, dan **tertiary_category** pada dataframe menjadi vektor TF-IDF secara terpisah menggunakan fit_transform.

- Menggabungkan Semua Fitur yang Sudah Divectorisasi:
Menggabungkan ketiga vektor hasil transformasi tersebut menjadi satu matriks fitur menggunakan fungsi hstack.

### Reviews Dataset (Collaborative Filtering)
"""

# Using selection feature for reviews dataset
reviews_df = reviews_df[['author_id','product_id','rating']]

# Checking for missing value
(reviews_df.isnull() |reviews_df.isna() | (reviews_df == "")).sum()

# Checking for duplicated data
reviews_df.duplicated().sum()

# Droping duplicated data
reviews_df.drop_duplicates(inplace=True)

# reviews shape
reviews_df.shape

#reviews uniqe character
reviews_df.nunique()

"""informasi awal dataset reviews berdimensi (49977,19) yang berari 49977 baris dengan 19 fitur. Dataset ini berisikan rating, ulasan produk dari author terhadap produk yang telah digunakan. Dari 19 fitur yang ada, dipilih 3 fitur antara lain sebagai berikut.

```
'author_id','product_id','rating'
```
3 fitur ini dipilih dikarenakan **author_id** mencirikan pengguna yang memberikan rating dan ulasan terhadapa produk yang digunakakan. **product_id** merepresentasikan nama produk yang dinilia serta, **rating** mencerminkan penilaian yang diberikan oleh author terhadap produk tersebut. 3 Fitur yang dipilih ini akan digunakan pada sistem rekomendasi **collaborative-filering**.

Setelah dilakukan pemilihan, dilakukan pengecheckan missing value, dan nilai yang terduplikasi. Ditemukan 0 missing value dan 33 data terdupliasi selanjutkan dilakukan drop pada nilai yang terduplikasi. Setelah dilakukan pembersihan tersisah 41457 row dengan karakter unik untuk product_id yang diulas sebanyak 1104 dan terdapat 5 tingkatan rating yang diberikan.
"""

# 1. Spliting data into train and test (before encoding)
train_df, test_df = train_test_split(reviews_df, test_size=0.2, random_state=42)

# 2. Encode user_id and product_id only based on the training data categories
user_cat = pd.Categorical(train_df['author_id'])
product_cat = pd.Categorical(train_df['product_id'])

train_user_ids = user_cat.codes
train_product_ids = product_cat.codes
train_ratings = train_df['rating'].values

# 3. Build a mapping from user_id/product_id to encoded index for later use in predictions
user_id_to_idx = dict(zip(user_cat.categories, range(len(user_cat.categories))))
product_id_to_idx = dict(zip(product_cat.categories, range(len(product_cat.categories))))

# 4. Create user-item rating matrix from training data
num_users = len(user_cat.categories)
num_products = len(product_cat.categories)
rating_matrix = np.zeros((num_users, num_products))

for u, p, r in zip(train_user_ids, train_product_ids, train_ratings):
    rating_matrix[u, p] = r

# Now rating_matrix contains training ratings only

# 5. For testing, encode test users/products using training encoders
def encode_user_product_ids(df, user_mapping, product_mapping):
    encoded_users = df['author_id'].map(user_mapping)
    encoded_products = df['product_id'].map(product_mapping)
    # Drop rows with unknown users/products (cold start)
    mask = encoded_users.notna() & encoded_products.notna()
    return encoded_users[mask].astype(int), encoded_products[mask].astype(int), df.loc[mask, 'rating']

test_user_ids, test_product_ids, test_ratings = encode_user_product_ids(test_df, user_id_to_idx, product_id_to_idx)

"""Langkah-langkah yang dilakukan pada proses ini untuk dataset revies adalah:

- Definisi Dataset:
  - Membuat variabel X yang terdiri dari kolom author_id dan product_id dari DataFrame reviews_df. Variabel ini mewakili fitur input untuk model.
  - Membuat variabel y yang merupakan kolom rating dari DataFrame yang sama. Variabel ini menjadi target output.
- encoding user dan product menjadi numeric
- membuat vektor yang berisikan user, item, dan rating
- spliting data training testing (80:20)

# Modeling

### Content-Based Learning
"""

# Counting Similarity Matrix
cosine_sim_matrix = cosine_similarity(combined_features)

cosine_sim_matrix

# Save cosine similarity matrix as dataframe
similarity_df = pd.DataFrame(cosine_sim_matrix, index=product_df['product_id'], columns=product_df['product_id'])

# Cosine similarity dimension
similarity_df.shape

# Reseting indices in product dataset
product_df = product_df.reset_index(drop=True)

# Getting Recomendation function
def generate_recommendations(item_id, num_recommendations=5):
    idx = product_df.index[product_df['product_id'] == item_id].tolist()[0]
    similarity_scores = cosine_sim_matrix[idx]
    sorted_indices = similarity_scores.argsort()[::-1]
    filtered_indices = [i for i in sorted_indices if i != idx][:num_recommendations]
    recommended_items = product_df.loc[filtered_indices, ['product_name', 'brand_name', 'primary_category', 'tertiary_category']].copy()
    recommended_items.insert(0, 'ranking', range(1, num_recommendations + 1))
    product_name = product_df.loc[product_df['product_id'] == item_id, 'product_name'].values[0]

    print(f"Recommendations for: {product_name}\n")
    print(tabulate(recommended_items, headers='keys', tablefmt='psql', showindex=False))

    return {
        'product_name': product_name,
        'recommendations': recommended_items
    }

def print_recommendations(rec_dict):
    print(f"Recommendations for: {rec_dict['product_name']}\n")
    print(tabulate(rec_dict['recommendations'], headers='keys', tablefmt='psql', showindex=False))

result = generate_recommendations('P473671')
print_recommendations(result)

"""### Collaborative Filtering"""

# Transpose rating matrix to get item vectors in rows
item_matrix = rating_matrix.T

# Compute cosine similarity between items
item_similarity = cosine_similarity(item_matrix)
print("Item similarity matrix shape:", item_similarity.shape)

def recommend_products_with_cosine_and_similar_users(user_id, review_data, product_data, n=5, m=5):
    if user_id not in review_data['author_id'].values:
        print(f"User {user_id} not found in the review data.")
        return

    # Create user-product rating matrix (pivot)
    user_product_matrix = review_data.pivot_table(index='author_id', columns='product_id', values='rating').fillna(0)

    # Compute cosine similarity matrices for items and users
    item_similarity_matrix = cosine_similarity(user_product_matrix.T)
    item_similarity_df = pd.DataFrame(item_similarity_matrix, index=user_product_matrix.columns, columns=user_product_matrix.columns)

    user_similarity_matrix = cosine_similarity(user_product_matrix)
    user_similarity_df = pd.DataFrame(user_similarity_matrix, index=user_product_matrix.index, columns=user_product_matrix.index)

    # Get products rated by the user
    user_ratings = review_data[review_data['author_id'] == user_id]
    rated_products = user_ratings['product_id'].tolist()

    all_products = user_product_matrix.columns.tolist()
    unrated_products = [p for p in all_products if p not in rated_products]

    # Calculate item-based recommendation scores
    scores = {}
    for rated_product in rated_products:
        if rated_product not in item_similarity_df.index:
            continue
        rated_product_rating = user_ratings[user_ratings['product_id'] == rated_product]['rating'].values[0]
        for unrated_product in unrated_products:
            if unrated_product not in item_similarity_df.columns:
                continue
            similarity_score = item_similarity_df.loc[rated_product, unrated_product]
            scores[unrated_product] = scores.get(unrated_product, 0) + similarity_score * rated_product_rating

    recommended_products = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:n]

    # Calculate user-based recommendations from similar users
    if user_id not in user_similarity_df.index:
        print(f"User {user_id} not found in the user similarity matrix.")
        similar_user_recommendations = []
    else:
        similar_users = user_similarity_df.loc[user_id].sort_values(ascending=False).drop(user_id).head(m).index.tolist()
        similar_user_scores = {}
        for sim_user in similar_users:
            sim_user_ratings = review_data[review_data['author_id'] == sim_user]
            for _, row in sim_user_ratings.iterrows():
                product = row['product_id']
                rating = row['rating']
                if product not in rated_products and rating >= 4:
                    similar_user_scores[product] = max(similar_user_scores.get(product, 0), rating)
        similar_user_recommendations = sorted(similar_user_scores.items(), key=lambda x: x[1], reverse=True)[:m]

    # Print item-based recommendations
    print(f"\nTop-{n} Item-based Recommendations for User {user_id}:\n")
    recommendations = []
    for rank, (product_id, score) in enumerate(recommended_products, start=1):
        product_info = product_data[product_data['product_id'] == product_id]
        if product_info.empty:
            continue
        product_name = product_info.iloc[0]['product_name']
        category = product_info.iloc[0]['primary_category']
        recommendations.append([rank, product_id, product_name, category, score])
    print(tabulate(recommendations, headers=["Rank", "Product ID", "Product Name", "Category", "Score"], tablefmt="psql"))

    # Print user-based recommendations
    print(f"\nTop-{m} User-based Recommendations from Similar Users for User {user_id}:\n")
    user_based_recs = []
    for rank, (product_id, rating) in enumerate(similar_user_recommendations, start=1):
        product_info = product_data[product_data['product_id'] == product_id]
        if product_info.empty:
            continue
        product_name = product_info.iloc[0]['product_name']
        category = product_info.iloc[0]['primary_category']
        user_based_recs.append([rank, product_id, product_name, category, rating])
    print(tabulate(user_based_recs, headers=["Rank", "Product ID", "Product Name", "Category", "Rating"], tablefmt="psql"))

recommend_products_with_cosine_and_similar_users('1845533064',reviews_df,product_df)

"""# Evaluation

### Content-Based Filtering
"""

def evaluate_product_recommendation(item_id, relevant_product_ids, product_df, cosine_sim_matrix, top_n=5):
    """
    Evaluate product recommendations using Precision@K.

    Parameters:
    - item_id: The ID of the target product to evaluate.
    - relevant_product_ids: A list of relevant product IDs (e.g., products in the same category).
    - product_df: DataFrame containing product information.
    - cosine_sim_matrix: Cosine similarity matrix between products.
    - top_n: Number of recommendations to evaluate.

    Returns:
    - A dictionary containing precision and additional evaluation details.
    """
    # Get the top-N recommendations for the target product
    recs = generate_recommendations(item_id, num_recommendations=top_n)
    recommended_df = recs['recommendations']

    # Extract product IDs from recommendations
    recommended_product_ids = recommended_df.index.to_list()
    recommended_product_ids = product_df.loc[recommended_product_ids, 'product_id'].tolist()

    # Calculate the intersection of recommended and relevant products
    hits = set(recommended_product_ids).intersection(set(relevant_product_ids))

    # Calculate Precision@K
    precision = len(hits) / top_n if top_n > 0 else 0

    print(f"Evaluation for Product ID: {item_id}")
    print(f"Precision@{top_n}: {precision:.4f}")
    print(f"Relevant Products: {relevant_product_ids}")
    print(f"Recommended Products: {recommended_product_ids}")

    return {
        'precision': precision,
        'hits': hits,
        'recommended_products': recommended_product_ids
    }


def evaluate_multiple_products(product_ids, product_df, cosine_sim_matrix, top_n=5):
    """
    Evaluate recommendations for multiple products and compute average Precision@K.

    Parameters:
    - product_ids: List of product IDs to evaluate.
    - product_df: DataFrame containing product information.
    - cosine_sim_matrix: Cosine similarity matrix between products.
    - top_n: Number of recommendations to evaluate for each product.

    Returns:
    - The average Precision@K for the evaluated products.
    """
    precision_scores = []

    for item_id in product_ids:
        # Define relevant products based on the primary category
        relevant_product_ids = product_df[
            product_df['primary_category'] == product_df.loc[product_df['product_id'] == item_id, 'primary_category'].values[0]
        ]['product_id'].tolist()
        relevant_product_ids = [pid for pid in relevant_product_ids if pid != item_id]  # Exclude the target item itself

        # Evaluate the product
        if relevant_product_ids:
            result = evaluate_product_recommendation(item_id, relevant_product_ids, product_df, cosine_sim_matrix, top_n=top_n)
            precision_scores.append(result['precision'])
        else:
            print(f"[SKIP] Product {item_id} does not have relevant products.")

    # Calculate the average Precision@K
    if precision_scores:
        avg_precision = np.mean(precision_scores)
        print(f"\nAverage Precision@{top_n} for {len(precision_scores)} products: {avg_precision:.4f}")
        return avg_precision
    else:
        print("No valid products for evaluation.")
        return 0


# Example Usage
sample_product_ids = product_df['product_id'].sample(5).tolist()  # Randomly sample 5 products
evaluate_multiple_products(sample_product_ids, product_df, cosine_sim_matrix, top_n=5)

"""### Collaborative Filtering"""

def recommendation_by_author(author_id, train_df, item_similarity, top_n=10):
    # Pivot train data to create the item-user matrix
    item_user_matrix = train_df.pivot_table(index='product_id', columns='author_id', values='rating', fill_value=0)

    # Check if the author_id exists in the matrix
    if author_id not in item_user_matrix.columns:
        print(f"[SKIP] Author {author_id} is not in the training data.")
        return pd.DataFrame(columns=['product_id', 'score'])

    # Get the ratings given by the user
    user_ratings = item_user_matrix[author_id]

    # Compute scores for all items
    scores = item_similarity.dot(user_ratings).reshape(-1)
    scores_df = pd.DataFrame({
        'product_id': item_user_matrix.index,
        'score': scores
    })

    # Exclude items the user has already rated
    rated_items = train_df[train_df['author_id'] == author_id]['product_id'].tolist()
    scores_df = scores_df[~scores_df['product_id'].isin(rated_items)]

    # Get the top-N recommendations
    recommended_df = scores_df.sort_values(by='score', ascending=False).head(top_n)

    return recommended_df

def evaluate_author_recommendation(author_id, train_df, test_df, item_similarity, k=10):
    actual_items = test_df[test_df['author_id'] == author_id]['product_id'].tolist()

    if not actual_items:
        print(f"[SKIP] Author {author_id} has no test data.")
        return None

    # Generate recommendations
    recommended_df = recommendation_by_author(author_id, train_df, item_similarity, top_n=k)

    if recommended_df.empty:
        print(f"[SKIP] No recommendations for Author {author_id}.")
        return None

    recommended_items = recommended_df['product_id'].tolist()

    # Calculate precision@k
    precision = len(set(actual_items) & set(recommended_items)) / k

    print(f"Author {author_id} | Precision@{k}: {precision:.4f}")
    print(f"Actual Items:      {actual_items}")
    print(f"Recommended Items: {recommended_items}")
    return precision

def evaluate_multiple_authors(author_ids, train_df, test_df, item_similarity, k=5):
    precision_scores = []

    for author_id in author_ids:
        score = evaluate_author_recommendation(author_id, train_df, test_df, item_similarity, k)
        if score is not None:
            precision_scores.append(score)

    if precision_scores:
        avg_precision = np.mean(precision_scores)
        print(f"\nAverage Precision@{k} for {len(precision_scores)} authors: {avg_precision:.4f}")
        return avg_precision
    else:
        print("No valid authors for evaluation.")
        return 0

# Example usage
sample_author_ids = test_df['author_id'].unique()[:5]
evaluate_multiple_authors(sample_author_ids, train_df, test_df, item_similarity, k=5)